{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V39 - Increased epoch from 8 to 10 , efficientnetB5 model with cutmix augmentation.\n",
    "\n",
    "\n",
    "v36 - adding customised switch function to provide 3 rotation augmentation and 2 cutmix augmentation passes . added genarized mean pooling in densenet model instead of globalized mean pooling.\n",
    "\n",
    "v35 - Using densenet for rotation augmentation and efficientnetb5 for cutmix augmentation . Reduced epoch from 15 to 8.\n",
    "\n",
    "V33 - Rerun with 15 epochs , 5 fold with two augmentation methods .LB-0.95727 CV - 0.96127\n",
    "\n",
    "v31 - Using switch value in the for loop with two different get_model function inorder to dynamically change the loss value based on the switch and augmentation method. \n",
    "\n",
    "V28,29,30 - Rerun which was a failure.\n",
    "\n",
    "\n",
    "v27 - Included rotation augmentation and based on random values , augmentation is performed for every epoch . Changed architecture from B6 to B5 . Increased folds from 3 to 5 and epochs from 10 to 15. \n",
    "\n",
    "v26 - Same config . Run for  3 folds. cv - 0.9723 LB - 0.95007 \n",
    "\n",
    "v22 - Increasing the probability of cutmix from 0.8 to 1 , changing the model from efficientnet B5 to B6 .Epochs increased from 3 to 5.\n",
    "\n",
    "V20: Reduced epochs from 15 to 10 ,number of folds from 5 to 3,batch size removed from 32 to 16 and removed the reduce learning rate on plateau .CV -0.9842  LB=0.95611\n",
    "\n",
    "V19: Adding external data -Oxford flowers TF record dataset by Chris to increase the training dataset. CV - 0.98358  LB - 0.95470\n",
    "\n",
    "V18:Adding generalized mean pooling with custom head inspired by [this](https://www.kaggle.com/bamps53/private0-9704-tpu-keras-metric-learning) kernel.  CV - 0.9620 , LB - 0.94552\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.1.0\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "# from tensorflow.keras import initializers\n",
    "# from tensorflow.keras import regularizers\n",
    "# from tensorflow.keras import constraints\n",
    "# from keras.engine import Layer, InputSpec\n",
    "import random\n",
    "#import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "import efficientnet.tfkeras as efn\n",
    "import albumentations\n",
    "from sklearn.metrics import f1_score,confusion_matrix,precision_score,recall_score\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "print(f'Tensorflow version {tf.__version__}')\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import math,re,os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    \n",
    "seed_everything(seed=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Starter Kernel , all the below codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "## Check if tpu is available:\n",
    "try:\n",
    "    tpu=tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print(\"Running on TPU\",tpu.master())\n",
    "except ValueError:\n",
    "    tpu=None\n",
    "    \n",
    "    \n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision enabled\n",
      "Accelerated Linear Algebra enabled\n"
     ]
    }
   ],
   "source": [
    "MIXED_PRECISION = True\n",
    "XLA_ACCELERATE = True\n",
    "\n",
    "if MIXED_PRECISION:\n",
    "    from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "    if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n",
    "    else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "    mixed_precision.set_policy(policy)\n",
    "    print('Mixed precision enabled')\n",
    "\n",
    "if XLA_ACCELERATE:\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    print('Accelerated Linear Algebra enabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check the files stored:\n",
    "!ls /kaggle/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://kds-00678dcb48cf1620abe5f19efdc3a50cd7949306c3125d21fa910615\n"
     ]
    }
   ],
   "source": [
    "## Competition data:\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path('flower-classification-with-tpus') # you can list the bucket with \"!gsutil ls $GCS_DS_PATH\"\n",
    "print(GCS_DS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For efficientnet:\n",
    "\n",
    "IMAGE_SIZE=[512,512]\n",
    "EPOCHS=10 ##  Increased from 8 to 10.\n",
    "BATCH_SIZE=8*strategy.num_replicas_in_sync\n",
    "AUG_BATCH = BATCH_SIZE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://kds-00678dcb48cf1620abe5f19efdc3a50cd7949306c3125d21fa910615/tfrecords-jpeg-512x512'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading competition data:\n",
    "GCS_PATH_SELECT = { # available image sizes\n",
    "    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n",
    "    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n",
    "    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n",
    "    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n",
    "}\n",
    "GCS_PATH=GCS_PATH_SELECT[IMAGE_SIZE[0]]\n",
    "GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls $GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join the oxford dataset in the training :\n",
    "TRAINING_FILENAMES=tf.io.gfile.glob(GCS_PATH+\"/train/*.tfrec\")\n",
    "VALIDATION_FILENAMES=tf.io.gfile.glob(GCS_PATH+\"/val/*.tfrec\")\n",
    "TEST_FILENAMES=tf.io.gfile.glob(GCS_PATH+\"/test/*.tfrec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n",
    "           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n",
    "           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n",
    "           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n",
    "           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n",
    "           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n",
    "           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n",
    "           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n",
    "           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n",
    "           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n",
    "           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']                                                                                                                                               # 100 - 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=15, linewidth=80)\n",
    "\n",
    "def batch_to_numpy_images_and_labels(data):\n",
    "    images,labels=data\n",
    "    numpy_images = images.numpy()\n",
    "    numpy_labels = labels.numpy()\n",
    "    if numpy_labels.dtype == object: # binary string in this case, these are image ID strings\n",
    "        numpy_labels = [None for _ in enumerate(numpy_images)]\n",
    "    # If no labels, only image IDs, return None for labels (this is the case for test data)\n",
    "    return numpy_images, numpy_labels\n",
    "\n",
    "def title_from_label_and_target(label, correct_label):\n",
    "    if correct_label is None:\n",
    "        return CLASSES[label], True\n",
    "    correct = (label == correct_label)\n",
    "    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n",
    "                                CLASSES[correct_label] if not correct else ''), correct\n",
    "\n",
    "def display_one_flower(image, title, subplot, red=False, titlesize=16):\n",
    "    plt.subplot(*subplot)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "    \n",
    "def display_batch_of_images(databatch, predictions=None):\n",
    "    \"\"\"This will work with:\n",
    "    display_batch_of_images(images)\n",
    "    display_batch_of_images(images, predictions)\n",
    "    display_batch_of_images((images, labels))\n",
    "    display_batch_of_images((images, labels), predictions)\n",
    "    \"\"\"\n",
    "    # data\n",
    "    images, labels = batch_to_numpy_images_and_labels(databatch)\n",
    "    if labels is None:\n",
    "        labels = [None for _ in enumerate(images)]\n",
    "        \n",
    "    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images)//rows\n",
    "        \n",
    "    # size and spacing\n",
    "    FIGSIZE = 13.0\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols,1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "    \n",
    "    # display\n",
    "    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n",
    "        title = '' if label is None else CLASSES[label]\n",
    "        correct = True\n",
    "        if predictions is not None:\n",
    "            title, correct = title_from_label_and_target(predictions[i], label)\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n",
    "        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n",
    "    \n",
    "    #layout\n",
    "    plt.tight_layout()\n",
    "    if label is None and predictions is None:\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    else:\n",
    "        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()\n",
    "    \n",
    "def display_confusion_matrix(cmat, score, precision, recall):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    ax = plt.gca()\n",
    "    ax.matshow(cmat, cmap='Reds')\n",
    "    ax.set_xticks(range(len(CLASSES)))\n",
    "    ax.set_xticklabels(CLASSES, fontdict={'fontsize': 7})\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "    ax.set_yticks(range(len(CLASSES)))\n",
    "    ax.set_yticklabels(CLASSES, fontdict={'fontsize': 7})\n",
    "    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    titlestring = \"\"\n",
    "    if score is not None:\n",
    "        titlestring += 'f1 = {:.3f} '.format(score)\n",
    "    if precision is not None:\n",
    "        titlestring += '\\nprecision = {:.3f} '.format(precision)\n",
    "    if recall is not None:\n",
    "        titlestring += '\\nrecall = {:.3f} '.format(recall)\n",
    "    if len(titlestring) > 0:\n",
    "        ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n",
    "    plt.show()\n",
    "    \n",
    "def display_training_curves(training, validation, title, subplot):\n",
    "    if subplot%10==1: # set up the subplots on the first call\n",
    "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
    "        plt.tight_layout()\n",
    "    ax = plt.subplot(subplot)\n",
    "    ax.set_facecolor('#F8F8F8')\n",
    "    ax.plot(training)\n",
    "    ax.plot(validation)\n",
    "    ax.set_title('model '+ title)\n",
    "    ax.set_ylabel(title)\n",
    "    #ax.set_ylim(0.28,1.05)\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'valid.'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image_data):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    label = tf.cast(example['class'], tf.int32)\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def read_unlabeled_tfrecord(example):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"id\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
    "        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = decode_image(example['image'])\n",
    "    idnum = example['id']\n",
    "    return image, idnum # returns a dataset of image(s)\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def data_augment(image, label):\n",
    "    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n",
    "    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n",
    "    # of the TPU while the TPU itself is computing gradients.\n",
    "    #image = tf.image.random_contrast(image,lower=0.3,upper=0.5,seed=101)\n",
    "    #image = tf.image.random_brightness(image, 0.1, seed=SEED)\n",
    "    image = tf.image.resize(image, [456,456])\n",
    "    #image = tf.image.resize(image, [299,299])  # inception_resnet_v2\n",
    "    #image = tf.image.random_saturation(image, 0, 2)\n",
    "    return image, label   \n",
    "\n",
    "def data_augment_valid(image, label):\n",
    "    #image = tf.image.resize(image, [300, 300])\n",
    "    image = tf.image.resize(image, [456,456])\n",
    "    return image, label   \n",
    "def data_augment_test(image, idnum):\n",
    "    #image = tf.image.resize(image, [300, 300])\n",
    "    image = tf.image.resize(image, [456,456])\n",
    "    return image,idnum\n",
    "\n",
    "def get_training_dataset(dataset,do_aug=False):\n",
    "    #dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n",
    "    #dataset=dataset.map()\n",
    "    #dataset=dataset.map(data_augment, num_parallel_calls=AUTO)\n",
    "    #switch = np.random.randn()\n",
    "    if do_cm:dataset=dataset.map(batch_cutmix,num_parallel_calls=AUTO)\n",
    "    if do_aug:dataset=dataset.map(data_augment,num_parallel_calls=AUTO)\n",
    "    dataset = dataset.shuffle(2048)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_validation_dataset(dataset,do_onehot=False,do_aug=False):\n",
    "    #dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    if do_onehot:dataset=dataset.map(one_hot_encode,num_parallel_calls=AUTO)\n",
    "    #switch = np.random.randint(50)\n",
    "#     if do_aug:\n",
    "#         if switch <=25 : \n",
    "#             print(f'Validation:Doing cutmix augmentation for switch value {switch}')\n",
    "#             dataset = dataset.map(cutmix_augment,num_parallel_calls=AUTO)\n",
    "#         elif switch <25 : \n",
    "#             print(f'Validation:Doing rotation transformation for switch value {switch}')\n",
    "#             dataset = dataset.map(transform,num_parallel_calls=AUTO)    \n",
    "    #if do_aug:dataset=dataset.map(data_augment,num_parallel_calls=AUTO)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(ordered=False,do_aug=False):\n",
    "    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n",
    "    if do_aug:dataset=dataset.map(data_augment,num_parallel_calls=AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 12753 training images, 3712 validation images, 7382 unlabeled test images,199 Steps per epoch \n"
     ]
    }
   ],
   "source": [
    "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
    "NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n",
    "NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n",
    "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
    "print('Dataset: {} training images, {} validation images, {} unlabeled test images,{} Steps per epoch '.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES, NUM_TEST_IMAGES,STEPS_PER_EPOCH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Dataset: 12753 training images, 3712 validation images, 7382 unlabeled test images,49 Steps per epoch .Lets train now with this increased data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspiration - [Chris kernel](https://www.kaggle.com/cdeotte/cutmix-and-mixup-on-gpu-tpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR scheduler from this kernel [Learning Rate Scheduler Code](https://www.kaggle.com/xhlulu/flowers-tpu-concise-efficientnet-b7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 1e-05 to 0.0004 to 1.56e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " #Learning rate schedule for TPU, GPU and CPU.\n",
    "# Using an LR ramp up because fine-tuning a pre-trained model.\n",
    "# Starting with a high LR would break the pre-trained weights.\n",
    "\n",
    "LR_START = 0.00001\n",
    "LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n",
    "LR_MIN = 0.00001\n",
    "LR_RAMPUP_EPOCHS = 5\n",
    "LR_SUSTAIN_EPOCHS = 0\n",
    "LR_EXP_DECAY = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "\n",
    "rng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    rotation = math.pi * rotation / 180.\n",
    "    shear = math.pi * shear / 180.\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "    c1 = tf.math.cos(rotation)\n",
    "    s1 = tf.math.sin(rotation)\n",
    "    one = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    rotation_matrix = tf.reshape( tf.concat([c1,s1,zero, -s1,c1,zero, zero,zero,one],axis=0),[3,3] )\n",
    "        \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)\n",
    "    shear_matrix = tf.reshape( tf.concat([one,s2,zero, zero,c2,zero, zero,zero,one],axis=0),[3,3] )    \n",
    "    \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = tf.reshape( tf.concat([one/height_zoom,zero,zero, zero,one/width_zoom,zero, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = tf.reshape( tf.concat([one,zero,height_shift, zero,one,width_shift, zero,zero,one],axis=0),[3,3] )\n",
    "    \n",
    "    return K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image,label):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image randomly rotated, sheared, zoomed, and shifted\n",
    "    DIM = IMAGE_SIZE[0]\n",
    "    XDIM = DIM%2 #fix for size 331\n",
    "    \n",
    "    rot = 15. * tf.random.normal([1],dtype='float32')\n",
    "    shr = 5. * tf.random.normal([1],dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
    "    w_zoom = 1.0 + tf.random.normal([1],dtype='float32')/10.\n",
    "    h_shift = 16. * tf.random.normal([1],dtype='float32') \n",
    "    w_shift = 16. * tf.random.normal([1],dtype='float32') \n",
    "  \n",
    "    # GET TRANSFORMATION MATRIX\n",
    "    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n",
    "\n",
    "    # LIST DESTINATION PIXEL INDICES\n",
    "    x = tf.repeat( tf.range(DIM//2,-DIM//2,-1), DIM )\n",
    "    y = tf.tile( tf.range(-DIM//2,DIM//2),[DIM] )\n",
    "    z = tf.ones([DIM*DIM],dtype='int32')\n",
    "    idx = tf.stack( [x,y,z] )\n",
    "    \n",
    "    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n",
    "    idx2 = K.dot(m,tf.cast(idx,dtype='float32'))\n",
    "    idx2 = K.cast(idx2,dtype='int32')\n",
    "    idx2 = K.clip(idx2,-DIM//2+XDIM+1,DIM//2)\n",
    "    \n",
    "    # FIND ORIGIN PIXEL VALUES           \n",
    "    idx3 = tf.stack( [DIM//2-idx2[0,], DIM//2-1+idx2[1,]] )\n",
    "    d = tf.gather_nd(image,tf.transpose(idx3))\n",
    "        \n",
    "    return tf.reshape(d,[DIM,DIM,3]),label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut Mix Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Idea from [Chris's](https://www.kaggle.com/cdeotte/cutmix-and-mixup-on-gpu-tpu) kernel . Research paper in [Arxiv](https://arxiv.org/abs/1905.04899)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cut mix to work , we need to convert the labels into one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(image,label):\n",
    "    CLASSES=104\n",
    "    return image,tf.one_hot(label,CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.cast(BATCH_SIZE*tf.math.sqrt(1-0.1),tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix_augment(image,label,prob=1):  # Change the prob of cutmix here\n",
    "    DIM=IMAGE_SIZE[0]\n",
    "    CLASSES=104\n",
    "    \n",
    "    imgs=[];labs=[]\n",
    "    for j in range(AUG_BATCH):\n",
    "        # Generate the probability:\n",
    "        P=tf.cast(tf.random.uniform([],0,1)<=prob,tf.int32)\n",
    "        #Generate a random image:\n",
    "        K=tf.cast(tf.random.uniform([],0,AUG_BATCH),tf.int32)\n",
    "        #Generate x,y coordinates:\n",
    "        x=tf.cast(tf.random.uniform([],0,DIM),tf.int32)\n",
    "        y=tf.cast(tf.random.uniform([],0,DIM),tf.int32)\n",
    "        #beta distribution:\n",
    "        b=tf.random.uniform([],0,1)\n",
    "        WIDTH=tf.cast(DIM*tf.math.sqrt(1-b),tf.int32)*P\n",
    "        ya=tf.math.maximum(0,y-WIDTH//2)\n",
    "        yb=tf.math.minimum(DIM,y+WIDTH//2)\n",
    "        xa=tf.math.maximum(0,x-WIDTH//2)\n",
    "        xb=tf.math.minimum(DIM,x+WIDTH//2)\n",
    "        \n",
    "        ##Make Cut Mix Image:\n",
    "        one=image[j,ya:yb,0:xa,:]   #images of size [n,dim,dim,3]\n",
    "        two=image[K,ya:yb,xa:xb,:]\n",
    "        three=image[j,ya:yb,xb:DIM,:]\n",
    "        middle=tf.concat([one,two,three],axis=1)\n",
    "        img=tf.concat([image[j,0:ya,:,:],middle,image[j,yb:DIM,:,:]],axis=0)\n",
    "        imgs.append(img)\n",
    "        ## Make the cutmix label:\n",
    "        a=tf.cast(WIDTH*WIDTH/DIM/DIM,tf.float32)\n",
    "        \n",
    "        if len(label.shape)==1:\n",
    "            lab1=tf.one_hot(label[j],CLASSES)\n",
    "            lab2=tf.one_hot(label[K],CLASSES)\n",
    "        \n",
    "        else:\n",
    "            lab1=label[j,]\n",
    "            lab2=label[K,]\n",
    "        labs.append((1-a)*lab1 + a*lab2)\n",
    "        \n",
    "        \n",
    "    image2=tf.reshape(tf.stack(imgs),(AUG_BATCH,DIM,DIM,3))\n",
    "    label2=tf.reshape(tf.stack(labs),(AUG_BATCH,CLASSES))\n",
    "    return image2,label2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of batch cutmix function from kernel - https://www.kaggle.com/yihdarshieh/batch-implementation-of-more-data-augmentations/notebook?scriptVersionId=29767726#Batch-CutMix\n",
    "def batch_cutmix(images, labels, PROBABILITY=1, batch_size=AUG_BATCH):\n",
    "    \n",
    "    DIM = IMAGE_SIZE[0]\n",
    "    CLASSES = 104\n",
    "    \n",
    "    if batch_size == 0:\n",
    "        batch_size = BATCH_SIZE\n",
    "    \n",
    "    # DO CUTMIX WITH PROBABILITY DEFINED ABOVE\n",
    "    # This is a tensor containing 0 or 1 -- 0: no cutmix.\n",
    "    # shape = [batch_size]\n",
    "    do_cutmix = tf.cast(tf.random.uniform([batch_size], 0, 1) <= PROBABILITY, tf.int32)\n",
    "    \n",
    "    # Choose random images in the batch for cutmix\n",
    "    # shape = [batch_size]\n",
    "    new_image_indices = tf.cast(tf.random.uniform([batch_size], 0, batch_size), tf.int32)\n",
    "    \n",
    "    # Choose random location in the original image to put the new images\n",
    "    # shape = [batch_size]\n",
    "    new_x = tf.cast(tf.random.uniform([batch_size], 0, DIM), tf.int32)\n",
    "    new_y = tf.cast(tf.random.uniform([batch_size], 0, DIM), tf.int32)\n",
    "    \n",
    "    # Random width for new images, shape = [batch_size]\n",
    "    b = tf.random.uniform([batch_size], 0, 1) # this is beta dist with alpha=1.0\n",
    "    new_width = tf.cast(DIM * tf.math.sqrt(1-b), tf.int32) * do_cutmix\n",
    "    \n",
    "    # shape = [batch_size]\n",
    "    new_y0 = tf.math.maximum(0, new_y - new_width // 2)\n",
    "    new_y1 = tf.math.minimum(DIM, new_y + new_width // 2)\n",
    "    new_x0 = tf.math.maximum(0, new_x - new_width // 2)\n",
    "    new_x1 = tf.math.minimum(DIM, new_x + new_width // 2)\n",
    "    \n",
    "    # shape = [batch_size, DIM]\n",
    "    target = tf.broadcast_to(tf.range(DIM), shape=(batch_size, DIM))\n",
    "    \n",
    "    # shape = [batch_size, DIM]\n",
    "    mask_y = tf.math.logical_and(new_y0[:, tf.newaxis] <= target, target <= new_y1[:, tf.newaxis])\n",
    "    \n",
    "    # shape = [batch_size, DIM]\n",
    "    mask_x = tf.math.logical_and(new_x0[:, tf.newaxis] <= target, target <= new_x1[:, tf.newaxis])    \n",
    "    \n",
    "    # shape = [batch_size, DIM, DIM]\n",
    "    mask = tf.cast(tf.math.logical_and(mask_y[:, :, tf.newaxis], mask_x[:, tf.newaxis, :]), tf.float32)\n",
    "\n",
    "    # All components are of shape [batch_size, DIM, DIM, 3]\n",
    "    new_images =  images * tf.broadcast_to(1 - mask[:, :, :, tf.newaxis], [batch_size, DIM, DIM, 3]) + \\\n",
    "                    tf.gather(images, new_image_indices) * tf.broadcast_to(mask[:, :, :, tf.newaxis], [batch_size, DIM, DIM, 3])\n",
    "\n",
    "    a = tf.cast(new_width ** 2 / DIM ** 2, tf.float32)    \n",
    "        \n",
    "    # Make labels\n",
    "    if len(labels.shape) == 1:\n",
    "        labels = tf.one_hot(labels, CLASSES)\n",
    "        \n",
    "    new_labels =  (1-a)[:, tf.newaxis] * labels + a[:, tf.newaxis] * tf.gather(labels, new_image_indices)        \n",
    "        \n",
    "    return new_images, new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how the augmentation works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:Doing cutmix augmentation\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-18-1cb8dda7c5e0>:32 transform  *\n        return tf.reshape(d,[DIM,DIM,3]),label\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:193 reshape\n        result = gen_array_ops.reshape(tensor, shape, name)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py:7443 reshape\n        \"Reshape\", tensor=tensor, shape=shape, name=name)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:742 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:595 _create_op_internal\n        compute_device)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:3322 _create_op_internal\n        op_def=op_def)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1786 __init__\n        control_input_ops)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1622 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Cannot reshape a tensor with 402653184 elements to shape [512,512,3] (786432 elements) for 'Reshape_5' (op: 'Reshape') with input shapes: [262144,512,3], [3] and with input tensors computed as partial shapes: input[1] = [512,512,3].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-8a5a1a4e1206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAUG_BATCH\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_elements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_training_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_FILENAMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdo_aug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0maugmented_elements\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_elements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUG_BATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcutmix_augment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-2347a165f595>\u001b[0m in \u001b[0;36mget_training_dataset\u001b[0;34m(dataset, do_aug)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUG_BATCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcutmix_augment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m       return ParallelMapDataset(\n\u001b[0;32m-> 1591\u001b[0;31m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3924\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3926\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3927\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[1;32m   3928\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 2395\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2396\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m   3139\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m       \u001b[0;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-18-1cb8dda7c5e0>:32 transform  *\n        return tf.reshape(d,[DIM,DIM,3]),label\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:193 reshape\n        result = gen_array_ops.reshape(tensor, shape, name)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py:7443 reshape\n        \"Reshape\", tensor=tensor, shape=shape, name=name)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:742 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:595 _create_op_internal\n        compute_device)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:3322 _create_op_internal\n        op_def=op_def)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1786 __init__\n        control_input_ops)\n    /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1622 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Cannot reshape a tensor with 402653184 elements to shape [512,512,3] (786432 elements) for 'Reshape_5' (op: 'Reshape') with input shapes: [262144,512,3], [3] and with input tensors computed as partial shapes: input[1] = [512,512,3].\n"
     ]
    }
   ],
   "source": [
    "rows=5\n",
    "cols=5\n",
    "\n",
    "rows=min(rows,AUG_BATCH//cols)\n",
    "all_elements=get_training_dataset(load_dataset(TRAINING_FILENAMES),do_aug=True).unbatch()\n",
    "\n",
    "augmented_elements=all_elements.repeat().batch(AUG_BATCH).map(cutmix_augment)\n",
    "\n",
    "for (img,label) in augmented_elements:\n",
    "    plt.figure(figsize=(15,int(15*rows/cols)))\n",
    "    for j in range(rows*cols):\n",
    "        plt.subplot(rows,cols,j+1)\n",
    "        plt.axis('off')\n",
    "        #print(img[j,].numpy().shape)\n",
    "        plt.imshow(img[j,])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows=5\n",
    "# cols=5\n",
    "\n",
    "# rows=min(rows,AUG_BATCH//cols)\n",
    "# all_elements=get_training_dataset(load_dataset(TRAINING_FILENAMES),do_cm=False).unbatch()\n",
    "\n",
    "# #augmented_elements=all_elements.repeat().batch(AUG_BATCH).map(cutmix_augment)\n",
    "# augmented_elements=all_elements.repeat().batch(AUG_BATCH).map(batch_cutmix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (img,label) in augmented_elements:\n",
    "#     plt.figure(figsize=(15,int(15*rows/cols)))\n",
    "#     for j in range(rows*cols):\n",
    "#         plt.subplot(rows,cols,j+1)\n",
    "#         plt.axis('off')\n",
    "#         #print(img[j,].numpy().shape)\n",
    "#         plt.imshow(img[j,])\n",
    "#         #print(CLASSES[label[j]])\n",
    "#     plt.show()\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (img,label) in augmented_elements.take(1):\n",
    "#     print(img.numpy().shape,label.numpy().shape)\n",
    "#     print(type(label),type(label.numpy()))\n",
    "#     print(label[1].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generalized_mean_pooling2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, p=3, epsilon=1e-6, name='', **kwargs):\n",
    "      super(Generalized_mean_pooling2D, self).__init__(name, **kwargs)\n",
    "      self.init_p = p\n",
    "      self.epsilon = epsilon\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "      if isinstance(input_shape, list) or len(input_shape) != 4:\n",
    "        raise ValueError('`GeM` pooling layer only allow 1 input with 4 dimensions(b, h, w, c)')\n",
    "      self.build_shape = input_shape\n",
    "      self.p = self.add_weight(\n",
    "              name='p',\n",
    "              shape=[1,],\n",
    "              initializer=tf.keras.initializers.Constant(value=self.init_p),\n",
    "              regularizer=None,\n",
    "              trainable=True,\n",
    "              dtype=tf.float32\n",
    "              )\n",
    "      self.built=True\n",
    "\n",
    "    def call(self, inputs):\n",
    "      input_shape = inputs.get_shape()\n",
    "      if isinstance(inputs, list) or len(input_shape) != 4:\n",
    "        raise ValueError('`GeM` pooling layer only allow 1 input with 4 dimensions(b, h, w, c)')\n",
    "      return (tf.reduce_mean(tf.abs(inputs**self.p), axis=[1,2], keepdims=False) + self.epsilon)**(1.0/self.p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model():\n",
    "\n",
    "#     with strategy.scope():\n",
    "#         pretrained_model=tf.keras.applications.inception_resnet_v2.InceptionResNetV2(input_shape=(512,512,3),include_top=False,weights=\"imagenet\")\n",
    "#         #pretrained_model.trainable = False # Removing the pretrained weights since pretrained weights give an categorical accuracy of max 0.34 . \n",
    "#         model=tf.keras.Sequential([pretrained_model,\n",
    "#                                    #tf.keras.layers.GlobalAveragePooling2D(),\n",
    "#                                    tf.keras.layers.GlobalMaxPool2D(),\n",
    "#                                    #tf.keras.layers.Dropout(0.3),\n",
    "#                                    #tf.keras.layers.Dense(1024,activation='elu'),\n",
    "#                                    #tf.keras.layers.Dropout(0.4),\n",
    "#                                   tf.keras.layers.Dense(len(CLASSES),activation=\"softmax\",dtype='float32')])\n",
    "#         #es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='auto',patience=5,verbose=1)\n",
    "#         #rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=5,verbose=1,mode='min',epsilon=0.0001)\n",
    "#     # lr_scheduler=tfa.optimizers.CyclicalLearningRate(initial_learning_rate=5e-5,\n",
    "#     #                                                 maximal_learning_rate=6e-3,\n",
    "#     #                                                 step_size=200,\n",
    "#     #                                                 scale_fn=lambda x:1.,\n",
    "#     #                                                 scale_mode=\"cycle\",\n",
    "#     #                                                 name=\"cyclic_learning_rate\")\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "#         loss = 'categorical_crossentropy',\n",
    "#         metrics=['categorical_accuracy']\n",
    "#     )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=efn.EfficientNetB5()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_model():\n",
    "\n",
    "#     with strategy.scope():\n",
    "#         pretrained_model=efn.EfficientNetB5(input_shape=(512,512,3),include_top=False,weights=\"noisy-student\") ## using noisy student weights instead of imagenet\n",
    "#         #pretrained_model.trainable = False \n",
    "#         model=tf.keras.Sequential([pretrained_model,\n",
    "#                                    #Generalized_mean_pooling2D(),\n",
    "#                                    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "#                                    #tf.keras.layers.GlobalMaxPool2D(),\n",
    "#                                    tf.keras.layers.Dropout(0.3),\n",
    "#                                    tf.keras.layers.Dense(1024,activation='relu'),\n",
    "#                                    #tf.keras.layers.Dropout(0.4),\n",
    "#                                   tf.keras.layers.Dense(len(CLASSES),activation=\"softmax\",dtype='float32')])\n",
    "#         #es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='auto',patience=5,verbose=1)\n",
    "#         #rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=5,verbose=1,mode='min',epsilon=0.0001)\n",
    "#     # lr_scheduler=tfa.optimizers.CyclicalLearningRate(initial_learning_rate=5e-5,\n",
    "#     #                                                 maximal_learning_rate=6e-3,\n",
    "#     #                                                 step_size=200,\n",
    "#     #                                                 scale_fn=lambda x:1.,\n",
    "#     #                                                 scale_mode=\"cycle\",\n",
    "#     #                                                 name=\"cyclic_learning_rate\")\n",
    "\n",
    "#     model.compile(\n",
    "#         optimizer='adam',\n",
    "#         loss = 'sparse_categorical_crossentropy',  ## for rotation augmentation require sparse_categorical crossentropy , for cutmix need categorical_crossentropy.\n",
    "#         metrics=['sparse_categorical_accuracy']\n",
    "#     )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "\n",
    "    with strategy.scope():\n",
    "        pretrained_model=tf.keras.applications.DenseNet201(input_shape=(512,512,3),include_top=False,weights=\"imagenet\") ## using noisy student weights instead of imagenet\n",
    "        #pretrained_model.trainable = False \n",
    "        model=tf.keras.Sequential([pretrained_model,\n",
    "                                   #Generalized_mean_pooling2D(),\n",
    "                                   tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                   #tf.keras.layers.GlobalMaxPool2D(),\n",
    "                                   tf.keras.layers.Dropout(0.3),\n",
    "                                   tf.keras.layers.Dense(1024,activation='relu'),\n",
    "                                   #tf.keras.layers.Dropout(0.4),\n",
    "                                  tf.keras.layers.Dense(len(CLASSES),activation=\"softmax\",dtype='float32')])\n",
    "        #es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='auto',patience=5,verbose=1)\n",
    "        #rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=5,verbose=1,mode='min',epsilon=0.0001)\n",
    "    # lr_scheduler=tfa.optimizers.CyclicalLearningRate(initial_learning_rate=5e-5,\n",
    "    #                                                 maximal_learning_rate=6e-3,\n",
    "    #                                                 step_size=200,\n",
    "    #                                                 scale_fn=lambda x:1.,\n",
    "    #                                                 scale_mode=\"cycle\",\n",
    "    #                                                 name=\"cyclic_learning_rate\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer= tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
    "        loss = 'sparse_categorical_crossentropy',  ## for rotation augmentation require sparse_categorical crossentropy , for cutmix need categorical_crossentropy.\n",
    "        metrics=['sparse_categorical_accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_cm():\n",
    "\n",
    "    with strategy.scope():\n",
    "        pretrained_model=efn.EfficientNetB5(input_shape=(512,512,3),include_top=False,weights=\"noisy-student\") ## using noisy student weights instead of imagenet\n",
    "        #pretrained_model.trainable = False \n",
    "        model=tf.keras.Sequential([pretrained_model,\n",
    "                                   #Generalized_mean_pooling2D(),\n",
    "                                   tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                                   #tf.keras.layers.GlobalMaxPool2D(),\n",
    "                                   tf.keras.layers.Dropout(0.3),\n",
    "                                   tf.keras.layers.Dense(1024,activation='relu'),\n",
    "                                   #tf.keras.layers.Dropout(0.4),\n",
    "                                  tf.keras.layers.Dense(len(CLASSES),activation=\"softmax\",dtype='float32')])\n",
    "        #es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='auto',patience=5,verbose=1)\n",
    "        #rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=5,verbose=1,mode='min',epsilon=0.0001)\n",
    "    # lr_scheduler=tfa.optimizers.CyclicalLearningRate(initial_learning_rate=5e-5,\n",
    "    #                                                 maximal_learning_rate=6e-3,\n",
    "    #                                                 step_size=200,\n",
    "    #                                                 scale_fn=lambda x:1.,\n",
    "    #                                                 scale_mode=\"cycle\",\n",
    "    #                                                 name=\"cyclic_learning_rate\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss = 'categorical_crossentropy',  ## for rotation augmentation require sparse_categorical crossentropy , for cutmix need categorical_crossentropy.\n",
    "        metrics=['categorical_accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(folds=5):\n",
    "    histories=[]\n",
    "    models=[]\n",
    "    all_preds=[]\n",
    "    all_probs=[]\n",
    "    scores=[]\n",
    "    es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='auto',patience=3,verbose=1) ## reducing patience from 5 to 3\n",
    "    rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5,patience=3,verbose=1,mode='min',epsilon=0.0001)\n",
    "    k_folds=KFold(folds,shuffle=True,random_state=SEED)\n",
    "    #model=get_model()\n",
    "    #model.summary()\n",
    "    #model=get_model() ## for switch<0.5,rotation augmentation\n",
    "    model=get_model_cm() ## for switch>0.5 , cutmix augmentation\n",
    "    for i,(trn_idx,val_idx) in enumerate(k_folds.split(TRAINING_FILENAMES)):\n",
    "        print(f\"---------------Folds {i+1}--------------\")\n",
    "        train_dataset=load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES':TRAINING_FILENAMES}).loc[trn_idx]['TRAINING_FILENAMES']),labeled=True)\n",
    "        \n",
    "        valid_dataset=load_dataset(list(pd.DataFrame({'TRAINING_FILENAMES':TRAINING_FILENAMES}).loc[val_idx]['TRAINING_FILENAMES']),labeled=True,ordered=True)\n",
    "        #checkpoint_name=f'efficientnetb3_{i+1}'+'.h5'\n",
    "        \n",
    "        #switch =np.random.uniform()## Taking it here since one hot encoding is required for cut mix.\n",
    "        switch=0.6\n",
    "        model_checkpoint=tf.keras.callbacks.ModelCheckpoint('switch-%s-model-%i.h5'%(switch,folds),\n",
    "                                                            monitor='val_loss',verbose=1,\n",
    "                                                            save_best_only=True,save_weights_only=True,save_freq='epoch')\n",
    "        \n",
    "#         if switch<0.5:\n",
    "#             #model=get_model()\n",
    "#             history= model.fit(get_training_dataset(train_dataset,do_aug=True,switch=switch), steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, validation_data=get_validation_dataset(valid_dataset,switch=switch),callbacks=[es,lr_callback,model_checkpoint])\n",
    "#             models.append(model)\n",
    "#             histories.append(history)\n",
    "#         else:\n",
    "#             #model=get_model_cm()\n",
    "#             history= model.fit(get_training_dataset(train_dataset,do_aug=True,switch=switch), steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, validation_data=get_validation_dataset(valid_dataset,switch=switch,do_onehot=True,do_aug=True),callbacks=[es,lr_callback,model_checkpoint])\n",
    "#             #models.append(model)\n",
    "#             #histories.append(history)\n",
    "        history= model.fit(get_training_dataset(train_dataset,do_aug=True,switch=switch), steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS, validation_data=get_validation_dataset(valid_dataset,switch=switch),callbacks=[es,lr_callback,model_checkpoint])\n",
    "        NUM_VALID_IMAGES=count_data_items(list(pd.DataFrame({'TRAINING_FILENAMES':TRAINING_FILENAMES}).loc[val_idx]['TRAINING_FILENAMES']))\n",
    "        print(f'Loading best weights for epoch {i+1}')\n",
    "        model.load_weights('switch-%s-model-%i.h5'%(switch,folds))\n",
    "        models.append(model)\n",
    "        histories.append(history)\n",
    "        cmdataset = get_validation_dataset(valid_dataset,switch=switch)\n",
    "        images_ds = cmdataset.map(lambda image, label: image)\n",
    "        labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n",
    "        \n",
    "        cm_correct_labels = next(iter(labels_ds.batch(NUM_VALID_IMAGES))).numpy() # get everything as one batch\n",
    "        cm_probabilities = model.predict(images_ds)\n",
    "        cm_predictions = np.argmax(cm_probabilities, axis=-1)\n",
    "\n",
    "        cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\n",
    "        score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
    "        scores.append(score)\n",
    "        print(f\"Macro F1 Score for epoch {i+1}:\",score)\n",
    "#         print(f\"---------Loss curve for epoch {i+1}---------\")\n",
    "#         display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211)\n",
    "#         print(f'---------Accuracy curve for epoch {i+1}---------')\n",
    "#         display_training_curves(history.history['categorical_accuracy'], history.history['val_categorical_accuracy'], 'accuracy', 212)\n",
    "        \n",
    "        \n",
    "        if FIRST_FOLD_ONLY:break\n",
    "    print(f'Average F1 score for all epochs:{np.average(scores)}')\n",
    "    return histories,models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_predict(folds=5):\n",
    "    test_dataset=get_test_dataset(ordered=True,do_aug=False)\n",
    "    test_images_ds=test_dataset.map(lambda image,idnum:image)\n",
    "    print(f\"------Starting Training for Folds {folds}-----\")\n",
    "    histories,models=train_model(folds=folds)\n",
    "    print(f'-----Computing Prediction---')\n",
    "     # get the mean probability of the folds models\n",
    "    if FIRST_FOLD_ONLY: probabilities = np.average([models[i].predict(test_images_ds) for i in range(1)], axis = 0)\n",
    "    else: probabilities = np.average([models[i].predict(test_images_ds) for i in range(folds)], axis = 0)\n",
    "    predictions = np.argmax(probabilities, axis=-1)\n",
    "    print('Generating submission.csv file...')\n",
    "    test_ids_ds = test_dataset.map(lambda image, idnum: idnum).unbatch()\n",
    "    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U') # all in one batch\n",
    "    np.savetxt('submission.csv', np.rec.fromarrays([test_ids, predictions]), fmt=['%s', '%d'], delimiter=',', header='id,label', comments='')\n",
    "    return histories, models\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "SEED=101\n",
    "#model=get_model()\n",
    "FIRST_FOLD_ONLY = False\n",
    "FOLDS=5\n",
    "histories,models=train_model_predict(folds=FOLDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
